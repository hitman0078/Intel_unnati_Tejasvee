{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82615096-d0ab-42c1-9f62-8db73e8030cb",
   "metadata": {},
   "source": [
    "# tiny-llama-1b-chat Chatbot using OpenVino\n",
    "### By- Tejasvee Dwivedi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a401dd-8b49-48ab-bb28-8a49ec636661",
   "metadata": {},
   "source": [
    "## Setting Up Environment and Installing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3172e676-032e-419b-8748-82d6af429982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "# %pip install -Uq pip\n",
    "# %pip uninstall -q -y optimum optimum-intel\n",
    "# %pip install --pre -Uq openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "# %pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
    "# \"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "# \"git+https://github.com/openvinotoolkit/nncf.git\"\\\n",
    "# \"torch>=2.1\"\\\n",
    "# \"datasets\" \\\n",
    "# \"accelerate\"\\\n",
    "# \"gradio>=4.19\"\\\n",
    "# \"onnx\" \"einops\" \"transformers_stream_generator\" \"tiktoken\" \"transformers>=4.38.1\" \"bitsandbytes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93f011-f8f9-494d-844a-894d940fdea1",
   "metadata": {},
   "source": [
    "## Configuring and Fetching the LLM Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d066f2f2-17a4-4fdf-a945-bb12f05cc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "# Define paths for the shared config file and destination config file\n",
    "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
    "config_dst_path = Path(\"llm_config.py\")\n",
    "\n",
    "def fetch_and_write_config(url, path):\n",
    "    # Fetch the config file from the given URL and write it to the specified path\n",
    "    response = requests.get(url)\n",
    "    with open(path, \"w\", encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "if not config_dst_path.exists():\n",
    "    if config_shared_path.exists():\n",
    "        try:\n",
    "            os.symlink(config_shared_path, config_dst_path)  # Try creating a symlink\n",
    "        except OSError:\n",
    "            shutil.copy(config_shared_path, config_dst_path)  # Copy if symlink fails\n",
    "    else:\n",
    "        fetch_and_write_config(\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\", config_dst_path)\n",
    "elif not os.path.islink(config_dst_path):\n",
    "    if config_shared_path.exists():\n",
    "        shutil.copy(config_shared_path, config_dst_path)  # Copy shared config if destination is not a symlink\n",
    "    else:\n",
    "        fetch_and_write_config(\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\", config_dst_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00398fd2-6959-4025-962d-51ae976c6de7",
   "metadata": {},
   "source": [
    "## Importing SUPPORTED_LLM_MODELS and Selecting Model Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68d38c79-66a3-48e7-9eac-86f59ec90b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8f78de04194b59a7eeef4b0b2f0dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model Language:', options=('English', 'Chinese', 'Japanese'), value='English')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_config import SUPPORTED_LLM_MODELS\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Get the list of supported LLM models\n",
    "model_languages = list(SUPPORTED_LLM_MODELS)\n",
    "\n",
    "# Create a dropdown widget for selecting model language\n",
    "model_language = widgets.Dropdown(\n",
    "    options=model_languages,\n",
    "    value=model_languages[0],  # Set default value\n",
    "    description=\"Model Language:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca75d13-2a69-4224-9126-6d33af671218",
   "metadata": {},
   "source": [
    "## Creating a Dropdown Widget for Selecting Model_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f279fd94-a036-46c5-807f-de3e300fd2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d0f621e92c424783d3b6bca33668fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', index=2, options=('qwen2-0.5b-instruct', 'tiny-llama-1b-chat', 'qwen2-1.5b-inst…"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of model IDs for the selected model language\n",
    "model_ids = list(SUPPORTED_LLM_MODELS[model_language.value])\n",
    "\n",
    "# Create a dropdown widget for selecting model ID\n",
    "model_id = widgets.Dropdown(\n",
    "    options=model_ids,\n",
    "    value=model_ids[2],  # Set default value\n",
    "    description=\"Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f8789e-553a-4db6-acd0-378df24ca10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model:- tiny-llama-1b-chat\n"
     ]
    }
   ],
   "source": [
    "model_configuration = SUPPORTED_LLM_MODELS[model_language.value][model_id.value]\n",
    "print(f\"Selected model:- {model_id.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64efc25f-4a9a-474f-a0bf-8477409cf1fd",
   "metadata": {},
   "source": [
    "## Creating Toggle Buttons for Model Preparation Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed6ea44e-e07e-4c0f-b71a-cc0763487609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ceb4d2bf21646eda480cd2a77e2fb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=True, description='Prepare INT4 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6873fff4604eefbbc21b0ae2074d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, description='Prepare INT8 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Toggle buttons for preparing INT4 and INT8 models\n",
    "prepare_int4_model = widgets.ToggleButton(\n",
    "    value=True,  # Initial state for INT4 model preparation\n",
    "    description=\"Prepare INT4 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_int8_model = widgets.ToggleButton(\n",
    "    value=False,  # Initial state for INT8 model preparation\n",
    "    description=\"Prepare INT8 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Display the toggle buttons\n",
    "display(prepare_int4_model)\n",
    "display(prepare_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60051433-74d9-4b7a-9bc1-7f0f5578eb31",
   "metadata": {},
   "source": [
    "## Generating and Executing Commands for Model Conversion to INT8 and INT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0904ef62-a83c-411e-82a5-bf5fed6f36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the model ID from the configuration\n",
    "pt_model_id = model_configuration[\"model_id\"]\n",
    "\n",
    "# Extract the model name from the selected model ID\n",
    "pt_model_name = model_id.value.split(\"-\")[0]\n",
    "\n",
    "# Define directories for INT8 and INT4 compressed models\n",
    "int8_model_dir = Path(pt_model_name) / \"INT8_compressed_weights\"\n",
    "int4_model_dir = Path(pt_model_name) / \"INT4_compressed_weights\"\n",
    "\n",
    "# Function to generate export command for OpenVINO\n",
    "def generate_export_command(model_id, task, weight_format, output_dir, remote_code=False, additional_args=\"\"):\n",
    "    base_command = f\"optimum-cli export openvino --model {model_id} --task {task} --weight-format {weight_format} {additional_args}\"\n",
    "    if remote_code:\n",
    "        base_command += \" --trust-remote-code\"\n",
    "    return f\"{base_command} {output_dir}\"\n",
    "\n",
    "# Function to convert model to INT8 format\n",
    "def convert_to_int8():\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    export_command = generate_export_command(pt_model_id, \"text-generation-with-past\", \"int8\", int8_model_dir, model_configuration.get(\"remote_code\", False))\n",
    "    display(Markdown(f\"**Export command:**\\n\\n`{export_command}`\"))\n",
    "    os.system(export_command)\n",
    "\n",
    "# Function to convert model to INT4 format\n",
    "def convert_to_int4():\n",
    "    compression_configs = {\n",
    "        \"default\": {\"sym\": False, \"group_size\": 128, \"ratio\": 0.8},\n",
    "    }\n",
    "    params = compression_configs.get(model_id.value, compression_configs[\"default\"])\n",
    "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    additional_args = f\"--group-size {params['group_size']} --ratio {params['ratio']}\"\n",
    "    if params[\"sym\"]:\n",
    "        additional_args += \" --sym\"\n",
    "    export_command = generate_export_command(pt_model_id, \"text-generation-with-past\", \"int4\", int4_model_dir, model_configuration.get(\"remote_code\", False), additional_args)\n",
    "    display(Markdown(f\"**Export command:**\\n\\n`{export_command}`\"))\n",
    "    os.system(export_command)\n",
    "\n",
    "# Check if INT8 model preparation is requested\n",
    "if prepare_int8_model.value:\n",
    "    convert_to_int8()\n",
    "\n",
    "# Check if INT4 model preparation is requested\n",
    "if prepare_int4_model.value:\n",
    "    convert_to_int4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f659a0d-ca4c-4be9-829f-3e484d232862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model with INT8 compressed weights is 1050.66 MB\n",
      "Size of model with INT4 compressed weights is 696.19 MB\n"
     ]
    }
   ],
   "source": [
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e427d1-5b6c-485a-85bc-af0fa161df00",
   "metadata": {},
   "source": [
    "## Listing Available OpenVINO Devices and Creating a Dropdown Widget for Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8742b31-6f2a-465e-a061-d718cffd4aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz\n",
      "GPU: Intel(R) Iris(R) Xe Graphics (iGPU)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b808e8b1907e4850abba02c171b46cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU'), value='CPU')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openvino as ov\n",
    "core = ov.Core()\n",
    "devices = core.available_devices\n",
    "\n",
    "# Loop through available devices and print their names\n",
    "for x in devices:\n",
    "    device_name = core.get_property(x, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{x}: {device_name}\")\n",
    "    \n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Create a dropdown widget for selecting device\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices,\n",
    "    value=core.available_devices[0], \n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ebe0e-4209-4f62-a9ea-63bd78dc094c",
   "metadata": {},
   "source": [
    "## Choice of Precision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72a76859-e4ab-4cdc-8749-d371b12a3403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f49766af5c4c3e912f4e3f52682798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model to run:', options=('INT4', 'INT8'), value='INT4')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models = []\n",
    "\n",
    "# Check if INT4 model directory exists and add to available models if true\n",
    "if int4_model_dir.exists():\n",
    "    available_models.append(\"INT4\")\n",
    "\n",
    "# Check if INT8 model directory exists and add to available models if true\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Create a dropdown widget for selecting model to run\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237c7a1-962c-4b50-abf0-ee76173dc6bf",
   "metadata": {},
   "source": [
    "## Loading and Configuring OpenVINO Model for Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30c46392-8ce6-4098-982c-04bf9e43bcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from tiny\\INT4_compressed_weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "\n",
    "# Determine model directory based on selected model type (INT4 or INT8)\n",
    "model_dir = int4_model_dir if model_to_run.value == \"INT4\" else int8_model_dir\n",
    "print(f\"Loading model from {model_dir}\")\n",
    "\n",
    "# OpenVINO configuration settings\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "# Load OpenVINO model for causal language modeling\n",
    "ov_model = OVModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device=device.value,  # Specify device for inference\n",
    "    ov_config=ov_config,  # OpenVINO configuration settings\n",
    "    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),  # Model configuration\n",
    "    trust_remote_code=True,  # Trust remote code when loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5661abd1-f1c7-4088-8694-b9bfca51af9e",
   "metadata": {},
   "source": [
    "## Generating Answer Using OpenVINO Model for Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b8f28e2-fec8-4fa1-8610-d16aa87ffdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is photosynthesis?\n",
      "Answer: photosynthesis is a reaction in which plants and chlorophyll cells use sunlight to convert atmospheric carbon dioxide and water into organic compounds (glucose) as well as oxygen gas in the process. During photosynthesis, the plants use light energy to split an electron acceptor (ATP) and release a light-absorbing molecule\n",
      "Time taken to generate the answer: 2.66 seconds\n"
     ]
    }
   ],
   "source": [
    "question = \"What is photosynthesis?\"\n",
    "prompt = f\"Question: {question}\\nAnswer:\"\n",
    "\n",
    "# Measure the time taken for generation\n",
    "start_time = time.time()\n",
    "input_tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "answer = ov_model.generate(input_ids=input_tokens['input_ids'], max_new_tokens=80, do_sample=True)\n",
    "end_time = time.time()\n",
    "\n",
    "# Decode the answer\n",
    "generated_answer = tokenizer.decode(answer[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_answer)\n",
    "print(f\"Time taken to generate the answer: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2672531-4622-4f4c-9cf2-fdcee8ee7d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  6 + 2 = \n",
      "Answer: 8\n",
      "Time taken to generate the answer: 0.18 seconds\n"
     ]
    }
   ],
   "source": [
    "question = \" 6 + 2 = \"\n",
    "prompt = f\"Question: {question}\\nAnswer:\"\n",
    "\n",
    "# Measure the time taken for generation\n",
    "start_time = time.time()\n",
    "input_tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "answer = ov_model.generate(input_ids=input_tokens['input_ids'], max_new_tokens=10, do_sample=True)\n",
    "end_time = time.time()\n",
    "\n",
    "# Decode the answer\n",
    "generated_answer = tokenizer.decode(answer[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_answer)\n",
    "print(f\"Time taken to generate the answer: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee2258-46f5-4364-88e4-6d0b67c93652",
   "metadata": {},
   "source": [
    "## Setting Up Conversational AI with OpenVINO Model Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38067254-401c-43b0-b8df-d48cb42561c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cf57ea9f-ceda-4284-8538-d8b2d6783fe0'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from threading import Event, Thread\n",
    "from uuid import uuid4\n",
    "from typing import List, Tuple\n",
    "import gradio as gr\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer,\n",
    ")\n",
    "\n",
    "# Define constants and configuration\n",
    "model_name = model_configuration[\"model_id\"]\n",
    "start_message = model_configuration[\"start_message\"]\n",
    "history_template = model_configuration.get(\"history_template\")\n",
    "current_message_template = model_configuration.get(\"current_message_template\")\n",
    "stop_tokens = model_configuration.get(\"stop_tokens\")\n",
    "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
    "\n",
    "max_new_tokens = 180\n",
    "\n",
    "# Define tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Define stop criteria\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stop_tokens = [StopOnTokens(tok.convert_tokens_to_ids(stop_tokens))] if stop_tokens else None\n",
    "\n",
    "# Define text processing function\n",
    "def default_partial_text_processor(partial_text: str, new_text: str):\n",
    "    partial_text += new_text\n",
    "    return partial_text\n",
    "\n",
    "text_processor = model_configuration.get(\"partial_text_processor\", default_partial_text_processor)\n",
    "\n",
    "# Define history conversion function\n",
    "def convert_history_to_token(history: List[Tuple[str, str]]):\n",
    "    if history_template is None:\n",
    "        messages = [{\"role\": \"system\", \"content\": start_message}]\n",
    "        for idx, (user_msg, model_msg) in enumerate(history):\n",
    "            if idx == len(history) - 1 and not model_msg:\n",
    "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "                break\n",
    "            if user_msg:\n",
    "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            if model_msg:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "        input_token = tok.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\")\n",
    "    else:\n",
    "        text = start_message + \"\".join([history_template.format(num=round, user=item[0], assistant=item[1]) for round, item in enumerate(history[:-1])])\n",
    "        text += \"\".join([current_message_template.format(num=len(history) + 1, user=history[-1][0], assistant=history[-1][1])])\n",
    "        input_token = tok(text, return_tensors=\"pt\", **tokenizer_kwargs).input_ids\n",
    "    return input_token\n",
    "\n",
    "# Define user callback function\n",
    "def user(message, history):\n",
    "    return \"\", history + [[message, \"\"]]\n",
    "\n",
    "# Define bot callback function\n",
    "def bot(history, temperature, top_p, top_k, repetition_penalty, conversation_id):\n",
    "    input_ids = convert_history_to_token(history)\n",
    "    streamer = TextIteratorStreamer(tok, timeout=30.0, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=temperature > 0.0,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    if stop_tokens:\n",
    "        generate_kwargs[\"stopping_criteria\"] = StoppingCriteriaList(stop_tokens)\n",
    "\n",
    "    stream_complete = Event()\n",
    "\n",
    "    def generate_and_signal_complete():\n",
    "        ov_model.generate(**generate_kwargs)\n",
    "        stream_complete.set()\n",
    "\n",
    "    t1 = Thread(target=generate_and_signal_complete)\n",
    "    t1.start()\n",
    "\n",
    "    partial_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        partial_text = text_processor(partial_text, new_text)\n",
    "        history[-1][1] = partial_text\n",
    "        yield history\n",
    "\n",
    "def request_cancel():\n",
    "    ov_model.request.cancel()\n",
    "\n",
    "def get_uuid():\n",
    "    return str(uuid4())\n",
    "get_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1876ce3f-0ab0-4380-91e7-68947cc9c190",
   "metadata": {},
   "source": [
    "## Creating Gradio Interface for Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "606f9d8a-f7d8-48cb-8a60-b3358ac22994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gradio interface\n",
    "with gr.Blocks(\n",
    "    theme='HaleyCH/HaleyCH_Theme',\n",
    "    css=\".gradio-container {background: linear-gradient(to bottom, #140225, #000000);\",\n",
    ") as demo:\n",
    "    conversation_id = gr.State(get_uuid)\n",
    "    gr.Markdown(f\"\"\"<h1><center>{model_id.value} Chatbot using OpenVino</center></h1>\"\"\")\n",
    "    gr.Markdown(f\"\"\"<h2><center>Made by:- Tejasvee Dwivedi</center></h2>\"\"\")\n",
    "    gr.Markdown(f\"\"\"<h5><center><a href=\"mailto:tejasvee.dwivedi@learner.manipal.edu\">tejasvee.dwivedi@learner.manipal.edu</a></center></h5>\"\"\")\n",
    "    gr.Markdown(f\"\"\"<h5><center>Manipal Institute of Technology</center></h5>\"\"\")\n",
    "    chatbot = gr.Chatbot(height=450)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Chat Message Box\",\n",
    "                placeholder=\"Chat Message Box\",\n",
    "                show_label=False,\n",
    "                container=False,\n",
    "            )\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                submit = gr.Button(\"Submit\")\n",
    "                stop = gr.Button(\"Stop\")\n",
    "                clear = gr.Button(\"Clear\")\n",
    "    with gr.Row():\n",
    "        with gr.Accordion(\"Advanced Options:\", open=False):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        temperature = gr.Slider(\n",
    "                            label=\"Temperature\",\n",
    "                            value=0.1,\n",
    "                            minimum=0.0,\n",
    "                            maximum=1.0,\n",
    "                            step=0.1,\n",
    "                            interactive=True,\n",
    "                            info=\"Higher values produce more diverse outputs\",\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        top_p = gr.Slider(\n",
    "                            label=\"Top-p (nucleus sampling)\",\n",
    "                            value=1.0,\n",
    "                            minimum=0.0,\n",
    "                            maximum=1,\n",
    "                            step=0.01,\n",
    "                            interactive=True,\n",
    "                            info=(\n",
    "                                \"Sample from the smallest possible set of tokens whose cumulative probability \"\n",
    "                                \"exceeds top_p. Set to 1 to disable and sample from all tokens.\"\n",
    "                            ),\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        top_k = gr.Slider(\n",
    "                            label=\"Top-k\",\n",
    "                            value=50,\n",
    "                            minimum=0.0,\n",
    "                            maximum=200,\n",
    "                            step=1,\n",
    "                            interactive=True,\n",
    "                            info=\"Sample from a shortlist of top-k tokens — 0 to disable and sample from all tokens.\",\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        repetition_penalty = gr.Slider(\n",
    "                            label=\"Repetition Penalty\",\n",
    "                            value=1.1,\n",
    "                            minimum=1.0,\n",
    "                            maximum=2.0,\n",
    "                            step=0.1,\n",
    "                            interactive=True,\n",
    "                            info=\"Penalize repetition — 1.0 to disable.\",\n",
    "                        )\n",
    "\n",
    "    submit_event = msg.submit(\n",
    "        fn=user,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=bot,\n",
    "        inputs=[\n",
    "            chatbot,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            repetition_penalty,\n",
    "            conversation_id,\n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        queue=True,\n",
    "    )\n",
    "    submit_click_event = submit.click(\n",
    "        fn=user,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=bot,\n",
    "        inputs=[\n",
    "            chatbot,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            repetition_penalty,\n",
    "            conversation_id,\n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        queue=True,\n",
    "    )\n",
    "    stop.click(\n",
    "        fn=request_cancel,\n",
    "        inputs=None,\n",
    "        outputs=None,\n",
    "        cancels=[submit_event, submit_click_event],\n",
    "        queue=False,\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7c282e0-e87a-4340-a22d-572db079f8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7862\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b645e-23f0-4b27-ac2c-9c165fa5ce23",
   "metadata": {},
   "source": [
    "## Fine tuning on certain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "904cbf11-864e-4593-9fcf-895cc06082ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ab2f170-e55b-426e-a80d-d48764fd8e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 17\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "file_path = \"D:\\Downloads\\GG.txt\"\n",
    "\n",
    "# Read the text file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    texts = file.readlines()\n",
    "\n",
    "# Remove newline characters and strip whitespace\n",
    "texts = [text.strip() for text in texts]\n",
    "\n",
    "# Create a Dataset object\n",
    "dataset = Dataset.from_dict({\n",
    "    'text': texts\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b39cc91-719c-467f-bc67-6cc10e79f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_configuration[\"model_id\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9ca9e97-ef60-4672-832e-94b586f26e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757394a8dff94510b7ce28557770373f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "56a67f60-b5a8-424f-9da0-6fdeb2aafe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                 # This is the directory where model checkpoints and outputs will be saved\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_steps=2000,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96373c9d-460c-47cf-9584-a33dfce022a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8d68c121-0493-45c9-ac57-a7df215f1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "17b17ff7-9ff2-45f4-a8f0-ec858975cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "class YourModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22ca0d-96c2-470b-aadf-a3ec18fa227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()trainer.train()\n",
    "model.save_pretrained('./fine-tuned-tinyllama-1b-chat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
